# Employee Data Engineering Pipeline

End-to-end data engineering project demonstrating real-world ETL, data quality, Spark processing, and Airflow orchestration.

## ğŸ”§ Tech Stack
- Python
- PostgreSQL
- Apache Spark (PySpark)
- Apache Airflow
- Docker
- Pandas

## ğŸ— Architecture
Raw Data â†’ Transform â†’ Data Quality â†’ Spark (Parquet Data Lake) â†’ PostgreSQL (UPSERT) â†’ Airflow Orchestration

## ğŸ“ Project Structure

employee_data_engineering/
â”œâ”€â”€ scripts/ # ETL, DQ, Spark, loaders
â”œâ”€â”€ data/ # Raw & processed data
â”œâ”€â”€ airflow_home/dags/ # Airflow DAGs
â”œâ”€â”€ docker-compose.yaml # Dockerized Airflow
â””â”€â”€ README.md

## ğŸš€ Key Features
- Step-2 data validation (profiling)
- Business rule transformations
- Automated data quality checks
- Incremental UPSERT into PostgreSQL
- Spark-based Parquet data lake
- Fully automated using Airflow (Dockerized)

## â–¶ How to Run
1. Clone the repository
2. Start Airflow using Docker:
   ```bash
   docker compose up -d
